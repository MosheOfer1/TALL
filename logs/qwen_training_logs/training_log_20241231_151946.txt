2024-12-31 15:19:46,387 - TrainingLogger - INFO - Using device: cuda
2024-12-31 15:19:46,387 - TrainingLogger - INFO - Model Architecture:
2024-12-31 15:19:46,387 - TrainingLogger - INFO - 
Detailed Model Architecture:
2024-12-31 15:19:46,388 - TrainingLogger - INFO - CustomLLM(
  (he_en_model_encoder): MarianEncoder(
    (embed_tokens): Embedding(60269, 1024, padding_idx=60268)
    (embed_positions): MarianSinusoidalPositionalEmbedding(1024, 1024)
    (layers): ModuleList(
      (0-5): 6 x MarianEncoderLayer(
        (self_attn): MarianAttention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (activation_fn): ReLU()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (embed_tokens): Embedding(151936, 896)
  (align_he_en): Sequential(
    (0): Linear(in_features=1024, out_features=1792, bias=True)
    (1): LayerNorm((1792,), eps=1e-05, elementwise_affine=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1792, out_features=896, bias=True)
    (5): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
  )
  (custom_decoder1): MarianDecoder(
    (embed_tokens): None
    (embed_positions): MarianSinusoidalPositionalEmbedding(1024, 896)
    (layers): ModuleList(
      (0-5): 6 x MarianDecoderLayer(
        (self_attn): MarianAttention(
          (k_proj): Linear(in_features=896, out_features=896, bias=True)
          (v_proj): Linear(in_features=896, out_features=896, bias=True)
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (out_proj): Linear(in_features=896, out_features=896, bias=True)
        )
        (activation_fn): GELUActivation()
        (self_attn_layer_norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MarianAttention(
          (k_proj): Linear(in_features=896, out_features=896, bias=True)
          (v_proj): Linear(in_features=896, out_features=896, bias=True)
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (out_proj): Linear(in_features=896, out_features=896, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=896, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=896, bias=True)
        (final_layer_norm): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (main_model): Qwen2Model(
    (embed_tokens): None
    (layers): ModuleList(
      (0-23): 24 x Qwen2DecoderLayer(
        (self_attn): Qwen2SdpaAttention(
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (k_proj): Linear(in_features=896, out_features=128, bias=True)
          (v_proj): Linear(in_features=896, out_features=128, bias=True)
          (o_proj): Linear(in_features=896, out_features=896, bias=False)
          (rotary_emb): Qwen2RotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
          (up_proj): Linear(in_features=896, out_features=4864, bias=False)
          (down_proj): Linear(in_features=4864, out_features=896, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((896,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (align_en_he): Sequential(
    (0): Linear(in_features=896, out_features=1024, bias=True)
    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (2): GELU(approximate='none')
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1024, out_features=512, bias=True)
    (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (custom_encoder2): MarianEncoder(
    (embed_tokens): None
    (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)
    (layers): ModuleList(
      (0-5): 6 x MarianEncoderLayer(
        (self_attn): MarianAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (activation_fn): SiLU()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (en_he_decoder): MarianDecoder(
    (embed_tokens): Embedding(65839, 512, padding_idx=65838)
    (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)
    (layers): ModuleList(
      (0-5): 6 x MarianDecoderLayer(
        (self_attn): MarianAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_fn): SiLU()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MarianAttention(
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (lm_head): Linear(in_features=512, out_features=65839, bias=False)
)
2024-12-31 15:19:46,389 - TrainingLogger - INFO - 
Total parameters: 799,239,680
2024-12-31 15:19:46,389 - TrainingLogger - INFO - Trainable parameters: 107,669,632
2024-12-31 15:19:46,389 - TrainingLogger - INFO - Percentage of trainable parameters: 13.47%
2024-12-31 15:19:46,390 - TrainingLogger - INFO - 
Detailed Layer-wise Information:
2024-12-31 15:19:46,390 - TrainingLogger - INFO - 
he_en_model_encoder:
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Total params: 138,341,376
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Trainable params: 0
2024-12-31 15:19:46,390 - TrainingLogger - INFO - 
embed_tokens:
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Total params: 136,134,656
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Trainable params: 0
2024-12-31 15:19:46,390 - TrainingLogger - INFO - 
align_he_en:
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Total params: 3,448,704
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Trainable params: 3,448,704
2024-12-31 15:19:46,390 - TrainingLogger - INFO -     In features: 1024
2024-12-31 15:19:46,390 - TrainingLogger - INFO -     Out features: 1792
2024-12-31 15:19:46,390 - TrainingLogger - INFO -     In features: 1792
2024-12-31 15:19:46,390 - TrainingLogger - INFO -     Out features: 896
2024-12-31 15:19:46,390 - TrainingLogger - INFO - 
custom_decoder1:
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Total params: 83,598,080
2024-12-31 15:19:46,390 - TrainingLogger - INFO -   Trainable params: 83,598,080
2024-12-31 15:19:46,391 - TrainingLogger - INFO - 
main_model:
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Total params: 357,898,112
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Trainable params: 0
2024-12-31 15:19:46,391 - TrainingLogger - INFO - 
align_en_he:
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Total params: 1,446,400
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Trainable params: 1,446,400
2024-12-31 15:19:46,391 - TrainingLogger - INFO -     In features: 896
2024-12-31 15:19:46,391 - TrainingLogger - INFO -     Out features: 1024
2024-12-31 15:19:46,391 - TrainingLogger - INFO -     In features: 1024
2024-12-31 15:19:46,391 - TrainingLogger - INFO -     Out features: 512
2024-12-31 15:19:46,391 - TrainingLogger - INFO - 
custom_encoder2:
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Total params: 19,176,448
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Trainable params: 19,176,448
2024-12-31 15:19:46,391 - TrainingLogger - INFO - 
en_he_decoder:
2024-12-31 15:19:46,391 - TrainingLogger - INFO -   Total params: 59,195,904
2024-12-31 15:19:46,392 - TrainingLogger - INFO -   Trainable params: 0
2024-12-31 15:19:46,392 - TrainingLogger - INFO - 
lm_head:
2024-12-31 15:19:46,392 - TrainingLogger - INFO -   Total params: 33,709,568
2024-12-31 15:19:46,392 - TrainingLogger - INFO -   Trainable params: 0
2024-12-31 15:19:46,393 - TrainingLogger - INFO - Detailed model summary has been saved to 'detailed_model_summary.txt'
2024-12-31 15:19:46,394 - TrainingLogger - INFO - Starting training from epoch 0
2024-12-31 15:19:53,267 - TrainingLogger - INFO - Step 10, Batch metrics: Loss: 9.8010, Accuracy: 0.0000, Perplexity: 18052.5215
2024-12-31 15:20:00,702 - TrainingLogger - INFO - Step 20, Batch metrics: Loss: 9.6689, Accuracy: 0.0058, Perplexity: 15818.4453
2024-12-31 15:20:07,855 - TrainingLogger - INFO - Step 30, Batch metrics: Loss: 9.2339, Accuracy: 0.0107, Perplexity: 10238.7945
2024-12-31 15:20:15,512 - TrainingLogger - INFO - Step 40, Batch metrics: Loss: 9.4758, Accuracy: 0.0186, Perplexity: 13040.0974
2024-12-31 15:20:23,130 - TrainingLogger - INFO - Step 50, Batch metrics: Loss: 9.1335, Accuracy: 0.0197, Perplexity: 9260.5895
2024-12-31 15:20:30,501 - TrainingLogger - INFO - Step 60, Batch metrics: Loss: 9.4751, Accuracy: 0.0156, Perplexity: 13030.5873
2024-12-31 15:20:37,736 - TrainingLogger - INFO - Step 70, Batch metrics: Loss: 9.0368, Accuracy: 0.0261, Perplexity: 8407.0037
2024-12-31 15:20:44,830 - TrainingLogger - INFO - Step 80, Batch metrics: Loss: 9.0121, Accuracy: 0.0355, Perplexity: 8201.8460
